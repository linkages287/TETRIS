# Analisi Problemi Algoritmo di Apprendimento
## Report Diagnostico e Soluzioni Proposte

**Data:** 2025-12-01  
**Modello:** `tetris_model.txt`

---

## üî¥ PROBLEMI CRITICI IDENTIFICATI

### 1. **Prestazioni in Degrado**
- **Average Score**: 2,794 ‚Üí **2,233** (-20.1% ‚ö†Ô∏è)
- **Best Score**: 33,062 (stagnante, non migliora)
- **Ratio Average/Best**: 8.5% ‚Üí **6.8%** (peggioramento)

**Implicazione**: Il modello sta **peggiorando** invece di migliorare. Questo indica:
- Instabilit√† nell'apprendimento
- Possibile overfitting o catastrophic forgetting
- Reward shaping non ottimale

### 2. **Bias2 Completamente Saturato**
- **Valore**: 19.95 (100% saturazione)
- **Range**: [-20.0, 20.0] (vicino al limite superiore)
- **Varianza**: 0.0 (nessuna variazione)

**Implicazione**: 
- Bias2 non pu√≤ pi√π apprendere (bloccato al limite)
- Limita la capacit√† del modello di adattarsi
- Riduce la flessibilit√† della rete neurale

### 3. **Alta Varianza in Weights2**
- **Varianza**: 17.45 (molto alta)
- **Range**: [-3.72, 4.30]
- **Std**: 2.34

**Implicazione**:
- Instabilit√† nell'apprendimento
- Pesi che oscillano troppo
- Possibile learning rate troppo alto

### 4. **Epsilon Management**
- **Epsilon corrente**: 0.1 (non pi√π al minimo)
- **Epsilon min**: 0.1
- **Stato**: Il modello √® tornato a esplorazione moderata

**Implicazione**:
- Il sistema ha rilevato il peggioramento e ha aumentato epsilon
- Ma questo potrebbe non essere sufficiente
- Potrebbe servire pi√π esplorazione strutturata

---

## üîç ANALISI CAUSE ROOT

### A. **Reward Shaping Subottimale**

**Problema attuale:**
```cpp
reward += lines_diff * 5.0;        // Troppo basso per line clearing
reward += score_diff * 0.1;        // Troppo basso per score
reward += 1.0;                     // Survival bonus troppo piccolo
reward -= 50.0;                    // Game over penalty troppo bassa
reward -= aggregate_height * 0.05; // Penalty troppo debole
reward -= holes * 0.3;             // Penalty troppo debole
```

**Problemi:**
1. Le ricompense sono troppo piccole rispetto alle penalit√†
2. Il modello non √® sufficientemente incentivato a fare line clears
3. Le penalit√† per stati cattivi sono troppo deboli
4. Il survival bonus √® troppo piccolo per guidare l'apprendimento

### B. **Q-Value Clipping Troppo Restrittivo**

**Problema attuale:**
```cpp
next_q = std::max(-100.0, std::min(100.0, next_q));
target = std::max(-100.0, std::min(100.0, target));
```

**Problemi:**
1. Range [-100, 100] potrebbe essere troppo stretto
2. Limita la capacit√† del modello di apprendere valori Q estremi
3. Pu√≤ causare saturazione prematura

### C. **Learning Rate Instabile**

**Problema attuale:**
- Learning rate: 0.003 (relativamente alto)
- Adaptive learning rate con logica complessa
- Pu√≤ causare oscillazioni

**Problemi:**
1. Learning rate troppo alto causa instabilit√† (varianza alta)
2. Adaptive logic potrebbe non essere ottimale
3. Non c'√® learning rate decay strutturato

### D. **Bias2 Update Logic**

**Problema attuale:**
- Bias2 ha learning rate aumentato (5x)
- Gradient clipping a [-5.0, 5.0]
- Weight decay aumentato (5x)
- Ma ancora saturato al limite

**Problemi:**
1. Le modifiche precedenti non hanno risolto la saturazione
2. Il clipping range [-20, 20] potrebbe essere troppo stretto
3. Il learning rate aumentato potrebbe causare oscillazioni

### E. **Experience Replay Buffer**

**Problema attuale:**
- Buffer size: 10,000
- Batch size: 32
- Sampling: 80% recent, 20% old

**Problemi:**
1. Bias verso esperienze recenti potrebbe causare catastrophic forgetting
2. Rimozione di game-over experiences potrebbe rimuovere informazioni importanti
3. Buffer potrebbe essere troppo piccolo per stabilit√†

---

## üí° SOLUZIONI PROPOSTE

### 1. **Migliorare Reward Shaping** ‚≠ê PRIORIT√Ä ALTA

**Obiettivo**: Guidare meglio l'apprendimento con ricompense pi√π bilanciate

**Modifiche proposte:**
```cpp
// Primary rewards (aumentati significativamente)
reward += lines_diff * 20.0;       // Aumentato da 5.0 (4x)
reward += score_diff * 0.5;        // Aumentato da 0.1 (5x)

// Survival bonus (aumentato)
if (!game.game_over) {
    reward += 2.0;                 // Aumentato da 1.0 (2x)
}

// Game over penalty (aumentata)
if (game.game_over) {
    reward -= 200.0;               // Aumentato da 50.0 (4x)
}

// State quality penalties (aumentate)
reward -= aggregate_height * 0.2;  // Aumentato da 0.05 (4x)
reward -= holes * 1.5;              // Aumentato da 0.3 (5x)
reward -= bumpiness * 0.1;          // Aumentato da 0.02 (5x)

// Bonus per board bassa (aumentato)
if (max_height < 10) {
    reward += 2.0;                 // Aumentato da 0.5 (4x)
}
```

**Razionalizzazione**:
- Line clearing √® l'obiettivo principale ‚Üí reward molto alto
- Score incrementale ‚Üí reward moderato ma significativo
- Survival ‚Üí reward costante per guidare l'apprendimento
- Penalties ‚Üí sufficientemente alte da evitare stati cattivi

### 2. **Risolvere Bias2 Saturation** ‚≠ê PRIORIT√Ä ALTA

**Obiettivo**: Permettere a bias2 di apprendere liberamente

**Modifiche proposte:**
```cpp
// Aumentare clipping range per bias2
const double BIAS2_MIN = -50.0;  // Aumentato da -20.0
const double BIAS2_MAX = 50.0;   // Aumentato da 20.0

// Ridurre learning rate per bias2 (pi√π stabile)
double bias2_learning_rate = learning_rate * 2.0;  // Ridotto da 5.0

// Ridurre gradient clipping per bias2
if (std::abs(bias2_gradient) > 3.0) {  // Ridotto da 5.0
    bias2_gradient = (bias2_gradient > 0) ? 3.0 : -3.0;
}

// Aggiungere momentum per bias2 (stabilit√†)
static double bias2_momentum = 0.0;
bias2_gradient = 0.9 * bias2_momentum + 0.1 * bias2_gradient;
bias2_momentum = bias2_gradient;
```

**Razionalizzazione**:
- Range pi√π ampio permette pi√π spazio per apprendere
- Learning rate pi√π basso riduce oscillazioni
- Momentum aggiunge stabilit√†
- Gradient clipping pi√π conservativo previene salti estremi

### 3. **Stabilizzare Learning Rate** ‚≠ê PRIORIT√Ä MEDIA

**Obiettivo**: Ridurre instabilit√† e varianza

**Modifiche proposte:**
```cpp
// Ridurre learning rate base
learning_rate = 0.0015;  // Ridotto da 0.003 (50%)

// Implementare learning rate decay strutturato
double lr_decay = 1.0 / (1.0 + training_episodes / 10000.0);
double current_lr = learning_rate * lr_decay;

// Usare learning rate pi√π conservativo per weights2
double weights2_lr = current_lr * 0.5;  // Met√† del learning rate normale
```

**Razionalizzazione**:
- Learning rate pi√π basso riduce varianza
- Decay strutturato permette apprendimento pi√π stabile
- Learning rate ridotto per weights2 previene oscillazioni

### 4. **Migliorare Q-Value Clipping** ‚≠ê PRIORIT√Ä MEDIA

**Obiettivo**: Permettere valori Q pi√π estremi senza instabilit√†

**Modifiche proposte:**
```cpp
// Aumentare range di clipping
next_q = std::max(-500.0, std::min(500.0, next_q));  // Aumentato da 100.0
target = std::max(-500.0, std::min(500.0, target));  // Aumentato da 100.0

// Oppure: clipping dinamico basato su reward massimo
double max_reward = 200.0;  // Basato su reward massimo possibile
double clip_range = max_reward * 10.0;  // 10x il reward massimo
next_q = std::max(-clip_range, std::min(clip_range, next_q));
```

**Razionalizzazione**:
- Range pi√π ampio permette valori Q pi√π realistici
- Previene saturazione prematura
- Mantiene stabilit√† con clipping appropriato

### 5. **Migliorare Experience Replay** ‚≠ê PRIORIT√Ä BASSA

**Obiettivo**: Migliorare stabilit√† dell'apprendimento

**Modifiche proposte:**
```cpp
// Aumentare buffer size
static const int BUFFER_SIZE = 20000;  // Aumentato da 10000

// Migliorare sampling strategy
// 60% recent, 40% old (pi√π bilanciato)
int recent_size = std::min((int)replay_buffer.size() / 2, ...);
int old_size = replay_buffer.size() - recent_size;

// Prioritized experience replay (opzionale)
// Sample esperienze con errore pi√π alto pi√π frequentemente
```

**Razionalizzazione**:
- Buffer pi√π grande = pi√π stabilit√†
- Sampling pi√π bilanciato previene catastrophic forgetting
- Prioritized replay accelera apprendimento

### 6. **Aggiungere Gradient Normalization** ‚≠ê PRIORIT√Ä MEDIA

**Obiettivo**: Prevenire gradient explosion e instabilit√†

**Modifiche proposte:**
```cpp
// Normalizzare gradienti prima di applicarli
double gradient_norm = 0.0;
for (auto& row : weights2) {
    gradient_norm += row[0] * row[0];
}
gradient_norm = std::sqrt(gradient_norm);

if (gradient_norm > 5.0) {  // Threshold
    double scale = 5.0 / gradient_norm;
    // Applicare scale a tutti i gradienti
}
```

**Razionalizzazione**:
- Previene gradient explosion
- Mantiene stabilit√† durante l'apprendimento
- Standard practice in deep learning

---

## üìä PRIORIT√Ä DI IMPLEMENTAZIONE

1. **‚≠ê‚≠ê‚≠ê CRITICO**: Migliorare Reward Shaping
2. **‚≠ê‚≠ê‚≠ê CRITICO**: Risolvere Bias2 Saturation
3. **‚≠ê‚≠ê ALTO**: Stabilizzare Learning Rate
4. **‚≠ê‚≠ê ALTO**: Migliorare Q-Value Clipping
5. **‚≠ê MEDIO**: Aggiungere Gradient Normalization
6. **‚≠ê BASSO**: Migliorare Experience Replay

---

## üéØ RISULTATI ATTESI

Dopo l'implementazione delle soluzioni:

1. **Average Score**: Dovrebbe aumentare da 2,233 a >3,000
2. **Consistenza**: Ratio Average/Best dovrebbe migliorare da 6.8% a >15%
3. **Bias2**: Dovrebbe uscire dalla saturazione (varianza > 0)
4. **Stabilit√†**: Varianza weights2 dovrebbe ridursi da 17.45 a <10
5. **Apprendimento**: Il modello dovrebbe mostrare trend positivo invece di negativo

---

## ‚ö†Ô∏è RISCHI E MITIGAZIONI

**Rischio 1**: Modifiche troppo aggressive potrebbero destabilizzare ulteriormente
- **Mitigazione**: Implementare modifiche gradualmente, testare dopo ogni cambio

**Rischio 2**: Reward shaping troppo alto potrebbe causare overfitting
- **Mitigazione**: Monitorare performance su validation set (se disponibile)

**Rischio 3**: Learning rate troppo basso potrebbe rallentare apprendimento
- **Mitigazione**: Usare learning rate decay invece di valore fisso basso

---

## üìù NOTE FINALI

Il modello mostra segni di instabilit√† e peggioramento. Le modifiche proposte sono mirate a:
1. Guidare meglio l'apprendimento (reward shaping)
2. Risolvere limitazioni strutturali (bias2 saturation)
3. Stabilizzare l'apprendimento (learning rate, gradient normalization)
4. Permettere pi√π flessibilit√† (Q-value clipping)

**Raccomandazione**: Implementare le modifiche in ordine di priorit√†, testando dopo ogni gruppo di modifiche.



